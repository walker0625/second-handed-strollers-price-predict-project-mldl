{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5771d4",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd76aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, Normalize, RandomRotation\n",
    "\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16d75e",
   "metadata": {},
   "source": [
    "## 이미지 Crop & Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a4b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가로/세로 비율 구하는 함수 \n",
    "def aspect_ratio(w, h):\n",
    "    return abs(w / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3979a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_crop_and_resize(img, diff_thresh = 0.33, \n",
    "                           pad_color=(114,114,114)): # 1:1에서 dev가 이 값 초과면 크롭(내용 보존↑면 0.33~0.35)\n",
    "    \"\"\"\n",
    "    - |w/h - 1| > diff_thresh면 중앙 '크롭', 아니면 '패딩'으로 정사각\n",
    "    - 그 다음에만 target_size로 리사이즈 (비율 왜곡 없음)\n",
    "    \"\"\"\n",
    "    # EXIF 방향 보정 + RGB\n",
    "    img = ImageOps.exif_transpose(img).convert(\"RGB\") # EXIF Orientation을 실제 픽셀에 반영해 올바른 방향으로 정규화\n",
    "\n",
    "    width, height = img.size\n",
    "    dev = abs(width / height - 1.0)\n",
    "\n",
    "    # 가로 세로 비율 차이가 크면 크롭을 적용합니다.\n",
    "    if dev > diff_thresh:\n",
    "        side = min(width, height)\n",
    "        left = (width - side) // 2\n",
    "        top  = (height - side) // 2\n",
    "        img = img.crop((left, top, left + side, top + side))\n",
    "    # 비율 차이가 작으면 패딩을 적용하여 정사각형으로 만듭니다.\n",
    "    else:\n",
    "        max_side = max(width, height)\n",
    "        new_img = Image.new('RGB', (max_side, max_side), pad_color)\n",
    "        new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n",
    "        img = new_img\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173c481",
   "metadata": {},
   "source": [
    "## 훈련용 / 테스트 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415a23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../csv/cleaned_total.csv')\n",
    "\n",
    "# 가격 이상치 정규화(한쪽으로 치우침 완하)\n",
    "df['price'] = np.log1p(df['price'])\n",
    "\n",
    "train_df = df[['price','image_path']].sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9045a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3346 836\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df)) # 훈련용 데이터 개수 / 테스트 데이터 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17ba29",
   "metadata": {},
   "source": [
    "## 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cf1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PirceImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe # CSV 불러온 데이터\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) # 데이터 개수\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(f'../../{row[\"image_path\"]}').convert(\"RGB\")  # 이미지를 RGB 이미지로 읽기\n",
    "        if self.transform:\n",
    "            img = self.transform(img) # 이미지 텐서형으로 변환\n",
    "        label = torch.tensor([row[\"price\"]], dtype=torch.float32) # 정답라벨을 가격데이터로 설정\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be18a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = Compose(\n",
    "    [ # (530, 690)\n",
    "        T.Lambda(lambda img: custom_crop_and_resize(img, aspect_ratio(720, 720))), # 이미지들의 평균 w, h 값\n",
    "        T.ToTensor(),\n",
    "        T.Resize((224, 224)), # VGG16의 권장된 resizing 값\n",
    "        RandomRotation(degrees=(-15, 15)), # 과적합 방지위해 이미지를 무작위 회전\n",
    "        RandomHorizontalFlip(p=0.5), # 과적합 방지위헤 50%로 좌우 대칭 변환\n",
    "        Normalize(mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = Compose(\n",
    "    [# (530, 690)\n",
    "        T.Lambda(lambda img: custom_crop_and_resize(img, aspect_ratio(720, 720))), # 이미지들의 평균 w, h 값\n",
    "        T.ToTensor(),\n",
    "        T.Resize((224, 224)), # VGG16의 권장된 resizing 값\n",
    "        Normalize(mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61e5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PirceImageDataset(train_df, transforms_train)\n",
    "test_dataset = PirceImageDataset(test_df, transforms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03de44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "title              0\n",
       "detail             1\n",
       "condition       1519\n",
       "is_completed       0\n",
       "price              0\n",
       "location        1005\n",
       "source             0\n",
       "model           2713\n",
       "model_type      2618\n",
       "log_price          0\n",
       "image_path         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162650",
   "metadata": {},
   "source": [
    "## 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43e35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32개의 이미지와 라벨을 묶어서 가져오기\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33ac42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\SecondHanded-Strollers-PredictedPrice\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Documents\\SecondHanded-Strollers-PredictedPrice\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 가져오기\n",
    "from torchvision.models.vgg import vgg16\n",
    "\n",
    "model = vgg16(pretrained=True) # 가중치값 가져오기\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee40b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전부 동결하기: 기존 가중치들은 업데이트 안한다\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "# 마지막 FC만 회귀용인 1차원으로 교체하고 학습 켜기\n",
    "in_feature = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(in_feature, 1)\n",
    "\n",
    "# 마지막 FC만 학습 켜기(마지막fc만 업데이트)\n",
    "for param in model.classifier[-1].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276115ea",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fb5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:51<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.57826828956604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:55<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1.279308795928955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:41<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.3686625957489014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:41<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.423297166824341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:50<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1.8263291120529175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  2.3449864387512207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:24<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.9807209968566895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:25<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.616286516189575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:26<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  5.3285813331604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:22<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  2.9419167041778564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:26<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.6766955852508545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:25<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  3.576709747314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:31<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  2.9252026081085205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:29<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1.7407076358795166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:23<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  2.946347951889038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train() # 학습 모드 켜기\n",
    "\n",
    "# optim = Adam(model.parameters(), lr=1e-3) # 최적화 함수\n",
    "optim = AdamW(model.parameters(), lr=0.0014432796537068773, weight_decay=0.0002337417039815359) # 최적화 함수 | 가중치 감쇠(정규화 항)\n",
    "criterion = nn.MSELoss() # 손실함수\n",
    "epochs = 15\n",
    "\n",
    "# 최적화 과정 시각화하기\n",
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in tqdm(train_dataloader): # 배차사이즈 만큼의 이미지와 라벨들\n",
    "        optim.zero_grad() # 한번 학습마다 최적화 함수인 기울기 초기화\n",
    "        preds = model(images.to(device)) # 순전파\n",
    "        loss = criterion(preds, labels.to(device)) # 손실계산\n",
    "\n",
    "        loss.backward() # 역전파로 각 파라미터의 기울기 계산\n",
    "        optim.step() # 기울기를 이용해 파라미터 업데이트\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "        step += 1\n",
    "    print(\"loss : \", loss.item()) # 현재 손실 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3d9397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:24<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 평가 결과:\n",
      "Metric     Value\n",
      "   MSE  4.254870\n",
      "   MAE  1.780848\n",
      "    R2 -3.840781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.regression import R2Score, MeanAbsoluteError\n",
    "\n",
    "# 모델 평가모드로 설정\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():        \n",
    "        # 평가 지표 객체들을 초기화합니다. 이 객체들은 기본적으로 CPU에 있습니다.\n",
    "        mse_metric = nn.MSELoss()\n",
    "        mae_metric = MeanAbsoluteError().to(device)\n",
    "        r2_metric = R2Score().to(device)\n",
    "        \n",
    "        total_mse_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(test_dataloader):\n",
    "            # 데이터를 GPU로 이동합니다.\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 순전파를 수행하여 예측값을 얻습니다.\n",
    "            preds = model(images)\n",
    "            \n",
    "            # total_mse_loss값을 계산을 위해 preds와 labels를 업데이트합니다. \n",
    "            mse_loss = mse_metric(preds, labels)\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            \n",
    "            mae_metric.update(preds, labels)\n",
    "            r2_metric.update(preds, labels)\n",
    "            \n",
    "        # 검증 데이터셋에 대한 최종 지표들을 계산합니다.\n",
    "        avg_mse_loss = total_mse_loss / len(test_dataloader)\n",
    "        final_mae = mae_metric.compute()\n",
    "        final_r2_score = r2_metric.compute()\n",
    "        \n",
    "        # 결과를 Pandas DataFrame으로 만들어 테이블 형태로 출력합니다.\n",
    "        results = pd.DataFrame({\n",
    "            'Metric': ['MSE', 'MAE', 'R2'],\n",
    "            'Value': [avg_mse_loss, final_mae.item(), final_r2_score.item()]\n",
    "        })\n",
    "        \n",
    "        print(\"\\n모델 평가 결과:\")\n",
    "        print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f9be",
   "metadata": {},
   "source": [
    "`uv run tensorboard --logdir=./src/training/runs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0eae4a",
   "metadata": {},
   "source": [
    "## 모델의 가중치만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb91d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/adamw_with_optuna.pth') # 딥러닝모델 저장 | 모델의 파라미터 값만 저장 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondHanded-Strollers-PredictedPrice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
